{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjaVbSZDwJtN"
   },
   "outputs": [],
   "source": [
    "# set up the config\n",
    "class Config:\n",
    "#     BATCH_SIZE = 16\n",
    "#     BATCH_SIZE = 8\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_LEN = 128\n",
    "    TARGET = 'category'\n",
    "    TEXT = 'text'\n",
    "    MODEL = 'distilbert-base-multilingual-cased'\n",
    "    LEARNING_RATE = 1e-05\n",
    "    EPOCHS = 5\n",
    "    EPS = 1e-08\n",
    "    random_seed = 0xfeedbeef\n",
    "    dataset_name = \"EENLP.NewsCatFull\"\n",
    "    dataset_file = \"CAT_NEWS.jsonl\"\n",
    "    train_language = 'English'\n",
    "    eval_languages = ('Armenian', 'Estonian', 'Latvian', 'Russian', 'Moldavian', 'Romanian', 'Slovak',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCtG46ABwMks",
    "outputId": "a73e707c-17d2-4ed7-c105-26e41e1557cb"
   },
   "outputs": [],
   "source": [
    "# prepare env\n",
    "\n",
    "!pip install transformers\n",
    "!pip install wget\n",
    "!pip install urllib2\n",
    "!pip install wandb -qqq\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7k_UZajwN0c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH9LlMiWwN8i",
    "outputId": "2f578952-3f6f-473d-8682-a6c62883a06e"
   },
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "      entity=\"eenlp\",\n",
    "      project=\"newscat_full\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config=dict([(k,v) for k,v in Config.__dict__.items() if k[0]!='_']),\n",
    "      reinit=True\n",
    ")\n",
    "#     run = wandb.init(project=\"storydb_eval.task3\", reinit=True)\n",
    "wandb.run.name += f'_{Config.MODEL}'\n",
    "wandb.run.save()    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "hVTOlxc8wN_i",
    "outputId": "d081e4bf-2a0a-4d27-a3dc-00fe65f9ee70"
   },
   "outputs": [],
   "source": [
    "label_encoder = None\n",
    "labels_codes = None\n",
    "\n",
    "def load_dataset(fn):\n",
    "    return pd.read_json(fn, lines=True) \n",
    "\n",
    "def get_train_and_test_split(ds, train_language):\n",
    "    tl_df = ds[ds['lang']==train_language]\n",
    "    df_test = tl_df[tl_df['split']=='test'].reset_index(drop=True)\n",
    "    df_train = tl_df[tl_df['split']=='train'].reset_index(drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "def get_eval_split(ds, eval_language):\n",
    "    el_df = ds[ds['lang']==eval_language].reset_index(drop=True)\n",
    "    return el_df\n",
    "\n",
    "def process_dataset(data, tokenizer, seq=False):\n",
    "    global label_encoder, labels_codes\n",
    "    if label_encoder is None:\n",
    "        print('init of label encoder')\n",
    "        label_encoder = LabelEncoder().fit(data[Config.TARGET])\n",
    "        keys = list(sorted(set(data[Config.TARGET])))\n",
    "        labels = label_encoder.transform(keys)\n",
    "        labels_codes = dict(zip(keys, labels))\n",
    "    data[Config.TARGET] = label_encoder.transform(data[Config.TARGET])\n",
    "\n",
    "    input_ids = torch.tensor([])\n",
    "    attention_masks = torch.tensor([])\n",
    "\n",
    "    for sent in data.loc[:, Config.TEXT]:\n",
    "        encoded_sent = tokenizer.encode_plus(sent, add_special_tokens = True,\n",
    "                                             max_length = Config.MAX_LEN, \n",
    "                                             padding = 'max_length',\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_tensors = 'pt')\n",
    "        input_ids = torch.cat([input_ids, encoded_sent['input_ids']])\n",
    "        attention_masks = torch.cat([attention_masks, encoded_sent['attention_mask']])\n",
    "    labels = torch.tensor(data[Config.TARGET])\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    if seq:\n",
    "        return DataLoader(dataset, sampler = SequentialSampler(dataset), batch_size = Config.BATCH_SIZE)\n",
    "    else:\n",
    "        return DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = Config.BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix PRNG\n",
    "random.seed(Config.random_seed)\n",
    "np.random.seed(Config.random_seed)\n",
    "torch.manual_seed(Config.random_seed)\n",
    "torch.cuda.manual_seed_all(Config.random_seed)\n",
    "\n",
    "# init tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL, truncation=True, do_lower_case=False)\n",
    "\n",
    "# load data frame\n",
    "fds = load_dataset(Config.dataset_file)\n",
    "\n",
    "# split english dataset\n",
    "print('parsing and preparing data, it will take a while.')\n",
    "print('english', end='... \\t')\n",
    "# train_df, test_df = load_dataset_and_split(Config.full_data, .8)\n",
    "train_df, test_df = get_train_and_test_split(fds, Config.train_language)\n",
    "train_loader = process_dataset(train_df, tokenizer, seq=False)\n",
    "eval_loaders = dict()\n",
    "eval_loaders[Config.train_language] = process_dataset(test_df, tokenizer, seq=True)\n",
    "print('done.')\n",
    "# prepare eval for other languages\n",
    "for lang in Config.eval_languages:\n",
    "    print(lang, end='... \\t')\n",
    "    eval_loaders[lang] = process_dataset(get_eval_split(fds, lang), tokenizer, seq=True)\n",
    "    print('done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   Config.MODEL,\n",
    "   num_labels = len(labels_codes),\n",
    "   output_attentions = False,\n",
    "   output_hidden_states = False    \n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, loader, scheduler=None):\n",
    "    model.train()\n",
    "\n",
    "    train_loss_accum = 0\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    \n",
    "    for index, (sentence, attention_mask, label) in tqdm(enumerate(loader)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence = sentence.to(device).long()\n",
    "        attention_mask = attention_mask.to(device).long()\n",
    "        label = label.to(device).long()\n",
    "\n",
    "        output = model(sentence, attention_mask = attention_mask, labels = label)\n",
    "        loss_value, logits = output[0], output[1]\n",
    "        train_loss_accum += loss_value.item()\n",
    "        fin_targets.extend(label.cpu().detach().numpy().tolist())\n",
    "        logits = logits.cpu().detach().numpy()\n",
    "        fin_outputs.extend(np.argmax(logits, axis=1))\n",
    "        \n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    avg_loss = train_loss_accum / index\n",
    "    train_accuracy = metrics.accuracy_score( fin_targets, fin_outputs )\n",
    "    train_f1_micro = metrics.f1_score(fin_targets, fin_outputs, average='micro')\n",
    "    train_f1_macro = metrics.f1_score(fin_targets, fin_outputs, average='macro')\n",
    "\n",
    "    wandb.log({\"train/loss\": avg_loss, \n",
    "               \"train/acc\":  train_accuracy,\n",
    "               \"train/f1_micro\" : train_f1_micro,\n",
    "               \"train/f1_macro\" : train_f1_macro,\n",
    "               \"epoch\":epoch,\n",
    "              })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for sentence, attention_mask, targets in testing_loader:\n",
    "            sentence = sentence.to(device).long()\n",
    "            attention_mask = attention_mask.to(device).long()\n",
    "            outputs = model(sentence, attention_mask = attention_mask)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            logits = outputs.logits.cpu().detach().numpy()\n",
    "            fin_outputs.extend(np.argmax(logits, axis=1))\n",
    "#             break\n",
    "    return fin_targets, fin_outputs\n",
    "\n",
    "def eval_model(model, epoch=-1):\n",
    "    for lang, eval_loader in eval_loaders.items():\n",
    "        targets, preds = validation(model, eval_loader)\n",
    "        scores = dict()\n",
    "        scores[f'valid/acc/{lang}'] = metrics.accuracy_score( targets, preds )\n",
    "        scores[f'valid/f1_micro/{lang}'] = metrics.f1_score(targets, preds, average='micro')\n",
    "        scores[f'valid/f1_macro/{lang}'] = metrics.f1_score(targets, preds, average='macro')\n",
    "        scores['epoch'] = epoch\n",
    "        print(scores)\n",
    "        wandb.log(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = Config.LEARNING_RATE, eps = Config.EPS)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, \n",
    "                                            num_training_steps = Config.EPOCHS*len(train_df)/Config.BATCH_SIZE)\n",
    "\n",
    "eval_model(model, epoch=-1)\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    train(model, epoch, train_loader, scheduler)\n",
    "    eval_model(model, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qolqX_cS1wjW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
