{
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "# set up the config\n",
    "class Config:\n",
    "#     BATCH_SIZE = 16\n",
    "    BATCH_SIZE = 8\n",
    "    MAX_LEN = 128\n",
    "    TARGET = 'label'\n",
    "    TEXT1 = 'sentence1'\n",
    "    TEXT2 = 'sentence2'\n",
    "    MODEL = 'distilbert-base-multilingual-cased'\n",
    "    LEARNING_RATE = 1e-05\n",
    "    EPOCHS = 5\n",
    "    EPS = 1e-08\n",
    "    random_seed = 0xfeedbeef\n",
    "    dataset = \"EENLP.ParaphraseDetection\"\n",
    "    dataset_version = \"v4\"\n",
    "    full_data = \"paraphrase_detection/english.jsonl\"\n",
    "    eval_data = {\n",
    "        \"armenian\": \"paraphrase_detection/armenian.jsonl\",\n",
    "        # \"belarusian\": \"paraphrase_detection/belarusian.jsonl\",\n",
    "        # \"bulgarian\": \"paraphrase_detection/bulgarian.jsonl\",\n",
    "        # \"croatian\": \"paraphrase_detection/croatian.jsonl\",\n",
    "        # \"czech\": \"paraphrase_detection/czech.jsonl\",\n",
    "        # \"english\": \"paraphrase_detection/english.jsonl\",\n",
    "        # \"estonian\": \"paraphrase_detection/estonian.jsonl\",\n",
    "        # \"hungarian\": \"paraphrase_detection/hungarian.jsonl\",\n",
    "        # \"lithuanian\": \"paraphrase_detection/lithuanian.jsonl\",\n",
    "        # \"macedonian\": \"paraphrase_detection/macedonian.jsonl\",\n",
    "        \"polish\": \"paraphrase_detection/polish.jsonl\",\n",
    "        \"romanian\": \"paraphrase_detection/romanian.jsonl\",\n",
    "        # \"russian\": \"paraphrase_detection/russian.jsonl\",\n",
    "        \"serbian\": \"paraphrase_detection/serbian.jsonl\",\n",
    "        # \"slovenian\": \"paraphrase_detection/slovenian.jsonl\",\n",
    "        # \"ukrainian\": \"paraphrase_detection/ukrainian.jsonl\",\n",
    "    }\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "XjaVbSZDwJtN"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "# Check if we have GPU\n",
    "!nvidia-smi"
   ],
   "outputs": [],
   "metadata": {
    "id": "PmcdCbZlpZuL"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "# prepare env\n",
    "\n",
    "!pip install transformers\n",
    "!pip install wget\n",
    "!pip install urllib2\n",
    "!pip install wandb -qqq\n",
    "!pip install jsonlines"
   ],
   "outputs": [],
   "metadata": {
    "id": "eCtG46ABwMks"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "z7k_UZajwN0c"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "# Log in to your W&B account\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "      entity=\"eenlp\",\n",
    "      project=\"paraphrase_detection\",\n",
    "      # Track hyperparameters and run metadata\n",
    "      config=dict([(k,v) for k,v in Config.__dict__.items() if k[0]!='_']),\n",
    "      reinit=True\n",
    ")\n",
    "#     run = wandb.init(project=\"storydb_eval.task3\", reinit=True)\n",
    "wandb.run.name += f'_{Config.MODEL}'\n",
    "wandb.run.save()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [],
   "metadata": {
    "id": "bH9LlMiWwN8i"
   }
  },
  {
   "source": [
    "wandb.run.use_artifact(f\"eenlp/paraphrase_detection/paraphrase_detection-dataset:{Config.dataset_version}\").download(\"paraphrase_detection\")"
   ],
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "id": "icrxU0PJrDi4"
   }
  },
  {
   "source": [
    "# limit number of examples per dataset, because otherwise training runs for a day\n",
    "\n",
    "df = pd.read_json(\"paraphrase_detection/english.jsonl\", lines=True)\n",
    "df = pd.concat([x[:10_000] for _, x in df.groupby(\"source\")])\n",
    "df.to_json(\"paraphrase_detection/english.jsonl\", orient=\"records\", lines=True)"
   ],
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "id": "sR-yl-zIjys4"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "label_encoder = None\n",
    "labels_codes = None\n",
    "\n",
    "def load_dataset(fn):\n",
    "    return pd.read_json(fn, lines=True) \n",
    "\n",
    "def load_dataset_and_split(fn, fraction=.8):\n",
    "    data = pd.read_json(fn, lines=True)\n",
    "    df_train=data.sample(frac=fraction,random_state=200)\n",
    "    df_test=data.drop(df_train.index).reset_index(drop=True)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "def process_dataset(data, tokenizer, seq=False):\n",
    "    global label_encoder, labels_codes\n",
    "    if label_encoder is None:\n",
    "        print('init of label encoder')\n",
    "        label_encoder = LabelEncoder().fit(data[Config.TARGET])\n",
    "        keys = list(sorted(set(data[Config.TARGET])))\n",
    "        labels = label_encoder.transform(keys)\n",
    "        labels_codes = dict(zip(keys, labels))\n",
    "    data[Config.TARGET] = label_encoder.transform(data[Config.TARGET])\n",
    "\n",
    "    input_ids = torch.tensor([])\n",
    "    attention_masks = torch.tensor([])\n",
    "\n",
    "    for sent1, sent2 in data.loc[:, [Config.TEXT1, Config.TEXT2]].itertuples(index=False):\n",
    "        # https://github.com/huggingface/transformers/blob/364a5ae1f0dc0f9098ff1ad4f5ede4a424813095/docs/source/task_summary.rst#sequence-classification\n",
    "        encoded_sent = tokenizer.encode_plus(sent1, sent2, add_special_tokens = True,\n",
    "                                             max_length = Config.MAX_LEN, \n",
    "                                             padding = 'max_length',\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_tensors = 'pt')\n",
    "        input_ids = torch.cat([input_ids, encoded_sent['input_ids']])\n",
    "        attention_masks = torch.cat([attention_masks, encoded_sent['attention_mask']])\n",
    "    labels = torch.tensor(data[Config.TARGET])\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    if seq:\n",
    "        return DataLoader(dataset, sampler = SequentialSampler(dataset), batch_size = Config.BATCH_SIZE)\n",
    "    else:\n",
    "        return DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = Config.BATCH_SIZE)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "hVTOlxc8wN_i"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "# fix PRNG\n",
    "random.seed(Config.random_seed)\n",
    "np.random.seed(Config.random_seed)\n",
    "torch.manual_seed(Config.random_seed)\n",
    "torch.cuda.manual_seed_all(Config.random_seed)\n",
    "\n",
    "# init tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL, truncation=True, do_lower_case=False)\n",
    "\n",
    "# split english dataset\n",
    "print('parsing and preparing data, it will take a while.')\n",
    "print('english', end='... \\t')\n",
    "train_df, test_df = load_dataset_and_split(Config.full_data, .8)\n",
    "train_loader = process_dataset(train_df, tokenizer, seq=False)\n",
    "eval_loaders = dict()\n",
    "eval_loaders['english'] = process_dataset(test_df, tokenizer, seq=True)\n",
    "print('done.')\n",
    "# prepare eval for other languages\n",
    "for lang, filename in Config.eval_data.items():\n",
    "    print(lang, end='... \\t')\n",
    "    eval_loaders[lang] = process_dataset(load_dataset(filename), tokenizer, seq=True)\n",
    "    print('done.')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "ak9XSnUwpZuO"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "# Init model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "   Config.MODEL,\n",
    "   num_labels = len(labels_codes),\n",
    "   output_attentions = False,\n",
    "   output_hidden_states = False    \n",
    ")\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {
    "id": "rfChdm5RpZuO"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "def train(model, epoch, loader, scheduler=None):\n",
    "    model.train()\n",
    "\n",
    "    train_loss_accum = 0\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    \n",
    "    for index, (sentence, attention_mask, label) in tqdm(enumerate(loader)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence = sentence.to(device).long()\n",
    "        attention_mask = attention_mask.to(device).long()\n",
    "        label = label.to(device).long()\n",
    "\n",
    "        output = model(sentence, attention_mask = attention_mask, labels = label)\n",
    "        loss_value, logits = output[0], output[1]\n",
    "        train_loss_accum += loss_value.item()\n",
    "        fin_targets.extend(label.cpu().detach().numpy().tolist())\n",
    "        logits = logits.cpu().detach().numpy()\n",
    "        fin_outputs.extend(np.argmax(logits, axis=1))\n",
    "        \n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    avg_loss = train_loss_accum / index\n",
    "    train_accuracy = metrics.accuracy_score( fin_targets, fin_outputs )\n",
    "    train_f1_micro = metrics.f1_score(fin_targets, fin_outputs, average='micro')\n",
    "    train_f1_macro = metrics.f1_score(fin_targets, fin_outputs, average='macro')\n",
    "\n",
    "    wandb.log({\"train/loss\": avg_loss,\n",
    "               \"train/acc\":  train_accuracy,\n",
    "               \"train/f1_micro\" : train_f1_micro,\n",
    "               \"train/f1_macro\" : train_f1_macro,\n",
    "               \"epoch\":epoch,\n",
    "              })\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "6_dG0XR8pZuP"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for sentence, attention_mask, targets in testing_loader:\n",
    "            sentence = sentence.to(device).long()\n",
    "            attention_mask = attention_mask.to(device).long()\n",
    "            outputs = model(sentence, attention_mask = attention_mask)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            logits = outputs.logits.cpu().detach().numpy()\n",
    "            fin_outputs.extend(np.argmax(logits, axis=1))\n",
    "#             break\n",
    "    return fin_targets, fin_outputs\n",
    "\n",
    "def eval_model(model, epoch=-1):\n",
    "    for lang, eval_loader in eval_loaders.items():\n",
    "        targets, preds = validation(model, eval_loader)\n",
    "        scores = dict()\n",
    "        scores[f'valid/acc/{lang}'] = metrics.accuracy_score( targets, preds )\n",
    "        scores[f'valid/f1_micro/{lang}'] = metrics.f1_score(targets, preds, average='micro')\n",
    "        scores[f'valid/f1_macro/{lang}'] = metrics.f1_score(targets, preds, average='macro')\n",
    "        scores['epoch'] = epoch\n",
    "        print(scores)\n",
    "        wandb.log(scores)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "cpTbdkhGpZuP"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(model.parameters(), lr = Config.LEARNING_RATE, eps = Config.EPS)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, \n",
    "                                            num_training_steps = Config.EPOCHS*len(train_df)/Config.BATCH_SIZE)\n",
    "\n",
    "eval_model(model, epoch=-1)\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    train(model, epoch, train_loader, scheduler)\n",
    "    eval_model(model, epoch)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "QfmeLKYnpZuQ"
   }
  },
  {
   "execution_count": null,
   "cell_type": "code",
   "source": [
    "wandb.run.finish()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pkBbiZTLpZuQ"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "name": "python",
   "file_extension": ".py",
   "version": "3.8.5",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  },
  "colab": {
   "provenance": [
    {
     "file_id": "10lsZu6qmGccCxp863VcfgSNEGMOwICVZ",
     "timestamp": 1641284155191
    }
   ],
   "collapsed_sections": [],
   "name": "task.paraphrase_detection.script.2.ipynb"
  },
  "accelerator": "GPU"
 }
}